{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import cv2\n",
                "import dlib\n",
                "import pickle\n",
                "import random\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "import tensorflow as tf\n",
                "from datetime import datetime\n",
                "from imutils import face_utils\n",
                "from tensorflow.keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dropout, Activation\n",
                "from tensorflow.keras.models import Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_dir = \".\"\n",
                "checkpoint_path = os.path.join(base_dir, 'logs/model/siamese-1')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "K = tf.keras.backend\n",
                "\n",
                "def preprocess_input(x, data_format=None, version=1):\n",
                "    x_temp = np.copy(x)\n",
                "    if data_format is None:\n",
                "        data_format = K.image_data_format()\n",
                "    assert data_format in {'channels_last', 'channels_first'}\n",
                "\n",
                "    if version == 1:\n",
                "        if data_format == 'channels_first':\n",
                "            x_temp = x_temp[:, ::-1, ...]\n",
                "            x_temp[:, 0, :, :] -= 93.5940\n",
                "            x_temp[:, 1, :, :] -= 104.7624\n",
                "            x_temp[:, 2, :, :] -= 129.1863\n",
                "        else:\n",
                "            x_temp = x_temp[..., ::-1]\n",
                "            x_temp[..., 0] -= 93.5940\n",
                "            x_temp[..., 1] -= 104.7624\n",
                "            x_temp[..., 2] -= 129.1863\n",
                "\n",
                "    elif version == 2:\n",
                "        if data_format == 'channels_first':\n",
                "            x_temp = x_temp[:, ::-1, ...]\n",
                "            x_temp[:, 0, :, :] -= 91.4953\n",
                "            x_temp[:, 1, :, :] -= 103.8827\n",
                "            x_temp[:, 2, :, :] -= 131.0912\n",
                "        else:\n",
                "            x_temp = x_temp[..., ::-1]\n",
                "            x_temp[..., 0] -= 91.4953\n",
                "            x_temp[..., 1] -= 103.8827\n",
                "            x_temp[..., 2] -= 131.0912\n",
                "    else:\n",
                "        raise NotImplementedError\n",
                "\n",
                "    return x_temp"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "vggface = tf.keras.models.Sequential()\n",
                "vggface.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
                "vggface.add(Convolution2D(64, (3, 3), activation='relu'))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(64, (3, 3), activation='relu'))\n",
                "vggface.add(MaxPooling2D((2,2), strides=(2,2)))\n",
                "vggface.add(ZeroPadding2D((1,1)))\t\n",
                "vggface.add(Convolution2D(128, (3, 3), activation='relu'))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(128, (3, 3), activation='relu'))\n",
                "vggface.add(MaxPooling2D((2,2), strides=(2,2)))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(256, (3, 3), activation='relu'))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(256, (3, 3), activation='relu'))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(256, (3, 3), activation='relu'))\n",
                "vggface.add(MaxPooling2D((2,2), strides=(2,2)))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(512, (3, 3), activation='relu'))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(512, (3, 3), activation='relu'))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(512, (3, 3), activation='relu'))\n",
                "vggface.add(MaxPooling2D((2,2), strides=(2,2)))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(512, (3, 3), activation='relu'))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(512, (3, 3), activation='relu'))\n",
                "vggface.add(ZeroPadding2D((1,1)))\n",
                "vggface.add(Convolution2D(512, (3, 3), activation='relu'))\n",
                "vggface.add(MaxPooling2D((2,2), strides=(2,2)))\n",
                "vggface.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
                "vggface.add(Dropout(0.5))\n",
                "vggface.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
                "vggface.add(Dropout(0.5))\n",
                "vggface.add(Convolution2D(2622, (1, 1)))\n",
                "vggface.add(Flatten())\n",
                "vggface.add(Activation('softmax'))\n",
                "\n",
                "vggface.pop()\n",
                "vggface.add(tf.keras.layers.Dense(128, use_bias=False))\n",
                "\n",
                "for layer in vggface.layers[:-2]:\n",
                "    layer.trainable = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SiameseNetwork(tf.keras.Model):\n",
                "    def __init__(self, vgg_face):\n",
                "        super(SiameseNetwork, self).__init__()\n",
                "        self.vgg_face = vgg_face\n",
                "        \n",
                "    @tf.function\n",
                "    def call(self, inputs):\n",
                "        image_1, image_2, image_3 =  inputs\n",
                "        with tf.name_scope(\"Anchor\") as scope:\n",
                "            feature_1 = self.vgg_face(image_1)\n",
                "            feature_1 = tf.math.l2_normalize(feature_1, axis=-1)\n",
                "        with tf.name_scope(\"Positive\") as scope:\n",
                "            feature_2 = self.vgg_face(image_2)\n",
                "            feature_2 = tf.math.l2_normalize(feature_2, axis=-1)\n",
                "        with tf.name_scope(\"Negative\") as scope:\n",
                "            feature_3 = self.vgg_face(image_3)\n",
                "            feature_3 = tf.math.l2_normalize(feature_3, axis=-1)\n",
                "        return [feature_1, feature_2, feature_3]\n",
                "    \n",
                "    @tf.function\n",
                "    def get_features(self, inputs):\n",
                "        return tf.math.l2_normalize(self.vgg_face(inputs), axis=-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SiameseNetwork(vggface)\n",
                "_ = model([tf.zeros((32,224,224,3)), tf.zeros((32,224,224,3)), tf.zeros((32,224,224,3))])\n",
                "_ = model.get_features(tf.zeros((32,224,224,3)))\n",
                "checkpoint = tf.train.Checkpoint(model=model)\n",
                "checkpoint.restore(checkpoint_path)\n",
                "data_dir = 'data/'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = input(\"Enter the name of the person : \")\n",
                "os.mkdir(os.path.join(data_dir,name))\n",
                "cap = cv2.VideoCapture(0)\n",
                "count = 0\n",
                "while True:\n",
                "    ret, frame = cap.read()\n",
                "    cv2.imshow('Image', frame)\n",
                "    k = cv2.waitKey(1)\n",
                "    if k == ord('s'):\n",
                "        cv2.imwrite(os.path.join(data_dir, name + '/' + str(count) + '.png') , frame)\n",
                "        count += 1\n",
                "    if k ==ord('q'):\n",
                "        break\n",
                "cap.release()\n",
                "cv2.destroyAllWindows()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "features = []\n",
                "people = sorted(os.listdir(data_dir))\n",
                "face_detector = dlib.get_frontal_face_detector()\n",
                "features = []\n",
                "dumpable_features = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "data/Aditya\n"
                    ]
                }
            ],
            "source": [
                "for person in people:\n",
                "    person_path = os.path.join(data_dir, person)\n",
                "    print(person_path)\n",
                "    images = []\n",
                "    for image in os.listdir(person_path):\n",
                "        image_path = os.path.join(person_path, image)\n",
                "        img = cv2.imread(image_path)\n",
                "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
                "        faces = face_detector(gray, 0)\n",
                "        if len(faces) == 1:\n",
                "            for face in faces:\n",
                "                face_bounding_box = face_utils.rect_to_bb(face)\n",
                "                if all(i >= 0 for i in face_bounding_box):\n",
                "                    [x, y, w, h] = face_bounding_box\n",
                "                    frame = img[y:y + h, x:x + w]\n",
                "                    frame = cv2.resize(frame, (224, 224))\n",
                "                    frame = np.asarray(frame, dtype=np.float64)\n",
                "                    images.append(frame)\n",
                "    images = np.asarray(images)\n",
                "    images = preprocess_input(images)\n",
                "    images = tf.convert_to_tensor(images)\n",
                "    feature = model.get_features(images)\n",
                "    feature = tf.reduce_mean(feature, axis=0)\n",
                "    features.append(feature.numpy())\n",
                "    dumpable_features[person] = feature.numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Aditya']\n"
                    ]
                }
            ],
            "source": [
                "features = np.asarray(features)\n",
                "print(people)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "cap = cv2.VideoCapture(0)\n",
                "count = 0\n",
                "name = 'not identified'\n",
                "while True:\n",
                "    ret, img = cap.read()\n",
                "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
                "    \n",
                "    faces = face_detector(gray, 0)\n",
                "    for face in faces:\n",
                "        face_bounding_box = face_utils.rect_to_bb(face)\n",
                "        if all(i >= 0 for i in face_bounding_box):\n",
                "            [x, y, w, h] = face_bounding_box\n",
                "            frame = img[y:y + h, x:x + w]\n",
                "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
                "            frame = cv2.resize(frame, (224, 224))\n",
                "            frame = np.asarray(frame, dtype=np.float64)\n",
                "            frame = np.expand_dims(frame, axis=0)\n",
                "            frame = preprocess_input(frame)\n",
                "            feature = model.get_features(frame)\n",
                "                \n",
                "            dist = tf.norm(features - feature, axis=1)\n",
                "            name = 'not identified'\n",
                "            loc = tf.argmin(dist)\n",
                "            if dist[loc] < 0.8:\n",
                "                name = people[loc]\n",
                "            else:\n",
                "#                     print(dist.numpy())\n",
                "                pass\n",
                "                    \n",
                "            font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
                "            cv2.putText(img, name, (x, y-5), font_face, 0.8, (0,0,255), 3)\n",
                "    cv2.imshow('Image', img)\n",
                "    k = cv2.waitKey(1)\n",
                "    if k ==ord('q'):\n",
                "        break\n",
                "cap.release()\n",
                "cv2.destroyAllWindows()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.2 ('base')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "a84dbc72f85724d673353a179bc750ef3d62fd504432d04d5e35c408d94a8430"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
